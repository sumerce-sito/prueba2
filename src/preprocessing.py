"""
MÃ³dulo de preprocesamiento de datos

Contiene funciones para:
- Carga de datos con manejo de tipos
- CreaciÃ³n de variable objetivo
- Limpieza y normalizaciÃ³n
- EliminaciÃ³n de data leakage
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Tuple, List, Optional
import warnings

from src.config import (
    DELAY_COLUMN,
    TARGET_COLUMN,
    DELAY_THRESHOLD,
    LEAKAGE_COLUMNS,
    MAX_ROWS_FOR_TRAINING,
)

warnings.filterwarnings('ignore')


def load_flight_data(
    filepath: Path,
    sample_size: Optional[int] = None,
    random_state: int = 42
) -> pd.DataFrame:
    """
    Carga el dataset de vuelos desde CSV con manejo optimizado de tipos.
    
    Importante para Google Colab:
    - Define explÃ­citamente dtype para columnas numÃ©ricas
    - Maneja columnas de fecha automÃ¡ticamente
    - Implementa sampling si el dataset es muy grande
    
    Args:
        filepath: Ruta al archivo CSV
        sample_size: NÃºmero mÃ¡ximo de filas a cargar (None = todas)
        random_state: Semilla para reproducibilidad del sampling
    
    Returns:
        DataFrame con los datos cargados
    """
    print(f"ðŸ“‚ Cargando datos desde: {filepath}")
    
    # Primero, leer solo las primeras filas para detectar columnas
    df_sample = pd.read_csv(filepath, nrows=1000)
    
    # Detectar columnas numÃ©ricas (posibles para conversiÃ³n)
    numeric_cols = df_sample.select_dtypes(include=[np.number]).columns.tolist()
    
    # Forzar que dep_delay sea numÃ©rico (crÃ­tico para crear el target)
    dtype_dict = {}
    if DELAY_COLUMN in df_sample.columns:
        dtype_dict[DELAY_COLUMN] = 'float64'
    
    # Cargar el dataset completo
    try:
        if sample_size:
            # Cargar con lÃ­mite de filas
            df = pd.read_csv(
                filepath,
                nrows=sample_size,
                dtype=dtype_dict,
                low_memory=False
            )
            print(f"âœ“ Datos cargados con lÃ­mite de {sample_size:,} registros")
        else:
            # Cargar todo el dataset
            df = pd.read_csv(
                filepath,
                dtype=dtype_dict,
                low_memory=False
            )
            print(f"âœ“ Datos cargados: {len(df):,} registros")
            
            # Si es muy grande, aplicar sampling estratificado
            if len(df) > MAX_ROWS_FOR_TRAINING:
                print(f"âš ï¸  Dataset muy grande ({len(df):,} registros)")
                print(f"   Aplicando sampling a {MAX_ROWS_FOR_TRAINING:,} registros...")
                df = df.sample(n=MAX_ROWS_FOR_TRAINING, random_state=random_state)
                print(f"âœ“ Sampling completado")
    
    except Exception as e:
        print(f"âŒ Error al cargar datos: {e}")
        raise
    
    return df


def normalize_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """
    Normaliza los nombres de columnas a formato estÃ¡ndar.
    
    - Convierte a minÃºsculas
    - Reemplaza espacios por guiones bajos
    - Elimina caracteres especiales
    
    Args:
        df: DataFrame original
    
    Returns:
        DataFrame con columnas normalizadas
    """
    df = df.copy()
    
    # Normalizar nombres
    df.columns = (
        df.columns
        .str.lower()
        .str.strip()
        .str.replace(' ', '_')
        .str.replace('[^a-z0-9_]', '', regex=True)
    )
    
    print(f"âœ“ Nombres de columnas normalizados")
    return df


def create_target_variable(df: pd.DataFrame) -> pd.DataFrame:
    """
    Crea la variable objetivo binaria 'is_delayed'.
    
    Regla:
    - is_delayed = 1 si dep_delay > DELAY_THRESHOLD (15 minutos)
    - is_delayed = 0 en caso contrario
    
    Args:
        df: DataFrame con columna de retraso
    
    Returns:
        DataFrame con columna TARGET_COLUMN aÃ±adida
    """
    df = df.copy()
    
    if DELAY_COLUMN not in df.columns:
        raise ValueError(f"âŒ Columna '{DELAY_COLUMN}' no encontrada en el dataset")
    
    # Crear variable binaria
    df[TARGET_COLUMN] = (df[DELAY_COLUMN] > DELAY_THRESHOLD).astype(int)
    
    # EstadÃ­sticas
    n_delayed = df[TARGET_COLUMN].sum()
    n_ontime = len(df) - n_delayed
    pct_delayed = n_delayed / len(df) * 100
    
    print(f"\nâœ“ Variable objetivo creada: '{TARGET_COLUMN}'")
    print(f"  Regla: retraso > {DELAY_THRESHOLD} minutos")
    print(f"  DistribuciÃ³n:")
    print(f"    - Puntuales (0): {n_ontime:,} ({100-pct_delayed:.1f}%)")
    print(f"    - Retrasados (1): {n_delayed:,} ({pct_delayed:.1f}%)")
    
    return df


def remove_leakage_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    Elimina columnas que causan data leakage.
    
    Data leakage: informaciÃ³n que no estarÃ­a disponible en el momento
    de hacer la predicciÃ³n (valores posteriores al evento).
    
    Args:
        df: DataFrame original
    
    Returns:
        DataFrame sin columnas de leakage
    """
    df = df.copy()
    
    # Filtrar solo las columnas que realmente existen
    cols_to_remove = [col for col in LEAKAGE_COLUMNS if col in df.columns]
    
    if cols_to_remove:
        df = df.drop(columns=cols_to_remove)
        print(f"\nâœ“ Columnas de data leakage eliminadas: {len(cols_to_remove)}")
        for col in cols_to_remove:
            print(f"    - {col}")
    else:
        print("\nâœ“ No se encontraron columnas de data leakage")
    
    return df


def handle_missing_values(df: pd.DataFrame, strategy: str = 'auto') -> pd.DataFrame:
    """
    Maneja valores nulos en el dataset.
    
    Estrategia 'auto':
    - NumÃ©ricas: mantener nulos (serÃ¡n imputados en el pipeline)
    - CategÃ³ricas: rellenar con 'missing'
    
    Args:
        df: DataFrame con posibles valores nulos
        strategy: Estrategia de imputaciÃ³n ('auto', 'drop', 'fill')
    
    Returns:
        DataFrame procesado
    """
    df = df.copy()
    
    # Reporte inicial de nulos
    null_counts = df.isnull().sum()
    cols_with_nulls = null_counts[null_counts > 0]
    
    if len(cols_with_nulls) == 0:
        print("\nâœ“ No se encontraron valores nulos")
        return df
    
    print(f"\nðŸ“Š Valores nulos encontrados en {len(cols_with_nulls)} columnas:")
    for col, count in cols_with_nulls.items():
        pct = count / len(df) * 100
        print(f"    - {col}: {count:,} ({pct:.1f}%)")
    
    if strategy == 'auto':
        # Rellenar categÃ³ricas con 'missing'
        categorical_cols = df.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            if df[col].isnull().sum() > 0:
                df[col] = df[col].fillna('unknown')
        
        print("\nâœ“ Valores nulos en categÃ³ricas rellenados con 'unknown'")
        print("  (Los nulos en numÃ©ricas se manejarÃ¡n en el pipeline)")
    
    elif strategy == 'drop':
        df = df.dropna()
        print(f"\nâœ“ Filas con nulos eliminadas. Registros restantes: {len(df):,}")
    
    return df


def detect_and_parse_dates(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
    """
    Detecta y parsea automÃ¡ticamente columnas de fecha/hora.
    
    Args:
        df: DataFrame original
    
    Returns:
        Tuple (DataFrame con fechas parseadas, lista de columnas de fecha)
    """
    df = df.copy()
    date_columns = []
    
    # Buscar columnas con patrones de fecha en el nombre
    potential_date_cols = [
        col for col in df.columns
        if any(keyword in col.lower() for keyword in ['date', 'time', 'day', 'year'])
    ]
    
    for col in potential_date_cols:
        try:
            # Intentar parsear como fecha
            df[col] = pd.to_datetime(df[col], errors='coerce')
            if df[col].dtype == 'datetime64[ns]':
                date_columns.append(col)
        except:
            continue
    
    if date_columns:
        print(f"\nâœ“ Columnas de fecha detectadas y parseadas: {date_columns}")
    
    return df, date_columns


def preprocess_data(
    filepath: Path,
    sample_size: Optional[int] = None,
    save_processed: bool = False,
    output_path: Optional[Path] = None
) -> pd.DataFrame:
    """
    Pipeline completo de preprocesamiento.
    
    Pasos:
    1. Cargar datos con tipos optimizados
    2. Normalizar nombres de columnas
    3. Detectar y parsear fechas
    4. Crear variable objetivo
    5. Eliminar columnas de data leakage
    6. Manejar valores nulos
    
    Args:
        filepath: Ruta al archivo CSV crudo
        sample_size: LÃ­mite de filas (None = todas)
        save_processed: Si guardar el resultado procesado
        output_path: Ruta para guardar (requerido si save_processed=True)
    
    Returns:
        DataFrame preprocesado
    """
    print("=" * 60)
    print("ðŸ”§ INICIANDO PREPROCESAMIENTO DE DATOS")
    print("=" * 60)
    
    # 1. Cargar datos
    df = load_flight_data(filepath, sample_size)
    
    # 2. Normalizar nombres
    df = normalize_column_names(df)
    
    # 3. Parsear fechas
    df, date_cols = detect_and_parse_dates(df)
    
    # 4. Crear variable objetivo
    df = create_target_variable(df)
    
    # 5. Eliminar data leakage
    df = remove_leakage_columns(df)
    
    # 6. Manejar nulos
    df = handle_missing_values(df, strategy='auto')
    
    # Resumen final
    print("\n" + "=" * 60)
    print("âœ… PREPROCESAMIENTO COMPLETADO")
    print("=" * 60)
    print(f"  ðŸ“Š Registros finales: {len(df):,}")
    print(f"  ðŸ“‹ Columnas finales: {len(df.columns)}")
    print(f"  ðŸŽ¯ Variable objetivo: '{TARGET_COLUMN}'")
    
    # Guardar si se solicita
    if save_processed and output_path:
        df.to_csv(output_path, index=False)
        print(f"\nðŸ’¾ Datos procesados guardados en: {output_path}")
    
    return df


if __name__ == "__main__":
    from src.config import get_raw_data_path, PROCESSED_DATA_DIR
    
    # Ejemplo de uso
    raw_path = get_raw_data_path()
    output_path = PROCESSED_DATA_DIR / "flight_data_processed.csv"
    
    df = preprocess_data(raw_path, save_processed=True, output_path=output_path)
    
    print("\nðŸ“‹ Primeras filas:")
    print(df.head())
